{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.099054,"end_time":"2022-08-13T13:43:32.105947","exception":false,"start_time":"2022-08-13T13:43:32.006893","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-23T09:23:51.972598Z","iopub.execute_input":"2022-08-23T09:23:51.973057Z","iopub.status.idle":"2022-08-23T09:23:51.990221Z","shell.execute_reply.started":"2022-08-23T09:23:51.973022Z","shell.execute_reply":"2022-08-23T09:23:51.988915Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Checking out the data i was given","metadata":{"papermill":{"duration":0.00928,"end_time":"2022-08-13T13:43:32.124811","exception":false,"start_time":"2022-08-13T13:43:32.115531","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n#import matplotlib.path as pat\nfrom PIL import Image, ImageDraw, ImageOps\nimport json\n#import timeit\nimport itertools\n#from shapely.geometry import Point, Polygon\nimport random\nimport cv2 as cv\n#import opendatasets as od\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\nimport torch.nn.functional as F\nimport torchvision\n\n\n#od.download(\"https://www.kaggle.com/competitions/hubmap-organ-segmentation/data\")\nmpath = '../input/'\nmpath_model = '../input/base-model'\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 1000)\npd.set_option('display.colheader_justify', 'center')\npd.set_option('display.precision', 3)\n\nplt.rcParams['figure.figsize'] = [15, 10]\n\n\ndf_train = pd.read_csv(mpath + '/hubmap-organ-segmentation/train.csv')\ndf_test = pd.read_csv(mpath + '/hubmap-organ-segmentation/test.csv')","metadata":{"papermill":{"duration":0.671527,"end_time":"2022-08-13T13:43:32.806072","exception":false,"start_time":"2022-08-13T13:43:32.134545","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-23T09:23:51.992912Z","iopub.execute_input":"2022-08-23T09:23:51.993720Z","iopub.status.idle":"2022-08-23T09:23:52.200326Z","shell.execute_reply.started":"2022-08-23T09:23:51.993665Z","shell.execute_reply":"2022-08-23T09:23:52.199099Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\nimport torch.nn.functional as F\n\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        #self.in_ch = in_ch\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch,out_ch,3,1,1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch,out_ch,3,1,1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=False),\n        )\n        \n    def forward(self,x):\n        #print(x.shape)#self.in_ch)\n        inter = self.conv(x) \n        #print(inter.shape)\n        return inter\n\nclass UNET(nn.Module):\n    def __init__(self,in_ch=3,out_ch=1,features=[64,128,256,512]):\n        super(UNET,self).__init__()\n        self.descend = nn.ModuleList()\n        self.ascend = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n        \n        for f in features:\n            self.descend.append(DoubleConv(in_ch,f))\n            in_ch = f\n            \n        for f in reversed(features):\n            self.ascend.append(nn.ConvTranspose2d(f*2,f,kernel_size=2,stride=2))\n            self.ascend.append(DoubleConv(f*2,f))\n            \n        self.bottleneck = DoubleConv(features[-1],features[-1]*2)\n        self.final_conv = nn.Conv2d(features[0],out_ch,kernel_size = 1)\n        \n    def forward(self,x):\n        skip_connections = []\n        #print(x.shape)\n        for d in self.descend:\n            #print(x.shape)\n            x = d(x)\n            #print(x.shape)\n            skip_connections.append(x)\n            #print(x.shape)\n            x = self.pool(x)\n            #print(x.shape)\n        #print(2)\n        x = self.bottleneck(x)\n        \n        skip_connections = skip_connections[::-1]\n        #print(3)\n        for idx in range(0,len(self.ascend),2):\n            x = self.ascend[idx](x)\n            skip_connection = skip_connections[idx//2]\n            #print(4+idx-1)\n            if x.shape != skip_connection.shape:\n                x = TF.resize(x,size=skip_connection.shape[2:])\n            \n            concat_skip = torch.cat((skip_connection, x),dim=1)\n            x = self.ascend[idx+1](concat_skip)\n            \n        return self.final_conv(x)\n    \ndef test():\n    x = torch.randn((3,1,161,161))\n    model = UNET(in_ch=1,out_ch=1)\n    preds = model(x)\n    print(preds.shape)\n    print(x.shape)\n    assert preds.shape == x.shape\n    \ntest()","metadata":{"papermill":{"duration":4.114193,"end_time":"2022-08-13T13:44:09.612368","exception":false,"start_time":"2022-08-13T13:44:05.498175","status":"completed"},"tags":[],"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-08-23T09:23:52.202011Z","iopub.execute_input":"2022-08-23T09:23:52.202426Z","iopub.status.idle":"2022-08-23T09:23:54.022771Z","shell.execute_reply.started":"2022-08-23T09:23:52.202382Z","shell.execute_reply":"2022-08-23T09:23:54.021493Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#loss from the interwebs\nclass DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n\n        #comment out if your model contains a sigmoid or equivalent activation layer\n        #inputs = F.sigmoid(inputs)\n\n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n\n        intersection = (inputs * targets).sum()\n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n\n        return 1 - dice","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-23T09:23:54.025135Z","iopub.execute_input":"2022-08-23T09:23:54.025548Z","iopub.status.idle":"2022-08-23T09:23:54.033208Z","shell.execute_reply.started":"2022-08-23T09:23:54.025510Z","shell.execute_reply":"2022-08-23T09:23:54.031849Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Data\n\nTODO:\n1. account for crops of images\n2. account for transforms of images(should be done on the fly vs preprocess (can slow down training) ","metadata":{"papermill":{"duration":0.077377,"end_time":"2022-08-13T13:44:09.766175","exception":false,"start_time":"2022-08-13T13:44:09.688798","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_crop_using_mask(im,w_slice=0,h_slice = 0,p_h = 0,p_w = 0):\n    \n    \n    x1 = 0\n    y1 = 0\n    x2 = im.shape[1]\n    y2 = im.shape[0]\n    #pad if the image is smaller than the pad amount + crop\n    h = w = False\n    \n    pad_front_w = pad_front_h = pad_back_w = pad_back_h = 0\n\n    x1 = x1-w_slice\n    if(x1<0):\n        pad_front_w = -x1\n        x1 = 0\n        w = True\n    y1 = y1-h_slice\n    if(y1<0):\n        pad_front_h = -y1\n        y1 = 0\n        h = True\n    #print(x2)\n    x2 = x2+(p_w-w_slice)\n    #print(x2)\n    if(x2>=im.shape[1]):\n        pad_back_w = x2-im.shape[1]\n        x2 = im.shape[1]\n        w = True\n    #print(y2)\n    y2 = y2+(p_h-h_slice)\n    #print(y2)\n    if(y2>=im.shape[0]):\n        pad_back_h = y2 - im.shape[0]\n        y2 = im.shape[0]\n        h = True\n    #print(x1,x2,y1,y2)\n    if(w or h):\n        #print(\"Padding fucked:\",pad_front_h,pad_back_h,pad_front_w,pad_back_w)\n        if(len(im.shape) == 3):\n            a = np.pad(im[y1:y2, x1:x2],pad_width=((pad_front_h,pad_back_h),(pad_front_w,pad_back_w),(0,0)),mode='reflect')\n        else:\n            a = np.pad(im[y1:y2, x1:x2],pad_width=((pad_front_h,pad_back_h),(pad_front_w,pad_back_w)),mode='reflect')\n    else:\n        a = im[y1:y2,x1:x2]\n\n    return a","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:23:54.035281Z","iopub.execute_input":"2022-08-23T09:23:54.036479Z","iopub.status.idle":"2022-08-23T09:23:54.050168Z","shell.execute_reply.started":"2022-08-23T09:23:54.036430Z","shell.execute_reply":"2022-08-23T09:23:54.048840Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#testing crop\na = np.array(Image.open('../input/hubmap-organ-segmentation/train_images/62.tiff'))\nprint(a.shape)\nim = get_crop_using_mask(a,w_slice=113,h_slice = 113,p_h = 227,p_w =227)\nprint(im.shape)\nplt.imshow(im)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:23:54.052773Z","iopub.execute_input":"2022-08-23T09:23:54.053994Z","iopub.status.idle":"2022-08-23T09:23:55.667740Z","shell.execute_reply.started":"2022-08-23T09:23:54.053943Z","shell.execute_reply":"2022-08-23T09:23:55.666560Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass HistoDataset(Dataset):\n    #crops will be rectangular \n    def __init__(self,im_dir,mask_dir,labels,transform=None,crop_h = -1,trans_dir = './otsu'):\n        self.im_dir = im_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.labels = pd.read_csv(labels)\n        self.no_mirror = 4\n        self.no_rot = 2\n        self.crop_h = crop_h\n        self.crop_w = crop_h\n        self.rand = random.Random()\n        rand.seed(2022)\n        self.path_otsu = trans_dir\n        initialise = False\n        \n        \n        #need to add the pre transformed images this should be probs done befor training to cut down on costs\n        \n        if crop_h == -1:\n            #we are using whole images\n            series = [1]*len(self.labels)\n            self.labels.insert(len(self.labels.columns),'len',series)\n        else:\n            #\n            #example of image path: ./otsu250im and mask: ./otsu250mask and df is ./otsu250df.pickle\n            #\n            p_im = Path(self.path_otsu+str(crop_h)+\"im\")\n            p_im.mkdir(parents=True, exist_ok=True)\n            m_im = Path(self.path_otsu+str(crop_h)+\"mask\")\n            m_im.mkdir(parents=True, exist_ok=True)\n            with os.scandir(p_im) as it:\n                initialise =  not any(it)\n            #we will be using crops\n            series = []\n            padding_h = []\n            padding_w = []\n            slice_w = []\n            slice_h = []\n            mask_included = []\n            crop_size = []\n            #indexes = []\n            #calc masks for tissue images and get the crop to calc amount of tiles(non overlaping with 0 padding)\n            #pr = False\n            if initialise:\n                for ix, im in self.labels.iterrows():\n                    #################!!!!!!!!!!!!!!!!!!!!!!!!!###################\n                    #       need to also shift the mask for padding amount      #\n                    #    either shift the mask points or just save bin masks    #\n                    #           mask will be saved as bin numpy array           #\n                    #          we also save the dataframe of the thing          #\n                    #############################################################\n\n                    #loading mask and image\n                    im_path = os.path.join(self.im_dir, str(im.id) + \".tiff\")\n                    mask_path = os.path.join(self.mask_dir, str(im.id) + \".json\") #to rabmo spremenit!!!! je JSON in ni actual slika\n                    image = Image.open(im_path).convert(\"RGB\")\n                    masks = json.load(open(mask_path))\n                    x,y = image.size\n                    mask = np.array(maskPIL((y,x),masks,to_array =False).convert(\"L\"),dtype=np.float32)\n                    mask[mask==255.0] = 1\n\n                    #\n                    #getting the otsu_mask and padding for crop fit\n                    m = otsu_mask_dilated(image,np.ones((10, 10), np.uint8),np.ones((300, 300), np.uint8))\n                    m_c = get_crop_of_mask(m)\n                    y_amount = m_c.shape[0] // self.crop_h #how many crops we can fit in without overlap, we want to get crops that include the tissue\n                    p_h = self.crop_h - m_c.shape[0] % self.crop_h\n                    x_amount = m_c.shape[1] // self.crop_w\n                    p_w = self.crop_w - m_c.shape[1] % self.crop_w\n                    im_len = self.no_mirror*self.no_rot*(y_amount + 1 if p_h>0 else 0 )*(x_amount + 1 if p_w>0 else 0 )\n                    s_w = self.rand.randint(int(p_w//4),int(3*p_w//4))\n                    s_h = self.rand.randint(int(p_h//4),int(3*p_h//4))\n                    series.append(im_len)\n                    padding_w.append(p_w)\n                    slice_w.append(s_w)\n                    padding_h.append(p_h)\n                    slice_h.append(s_h)\n                    #crop_size.append(m_c.shape)\n\n                    #\n                    #creating the crops\n                    mask = get_crop_using_mask(m,mask,w_slice = s_w,h_slice = s_h,p_h = p_h,p_w = p_w)\n\n                    #TODO check which crops hold the mask to have a well balanced train val split!\n                    \"\"\"for crop in range(im_len):\n                        self.convert_index_to_transform(crop)\n                        if np.sum(mask) > 0:\"\"\"\n\n                    image = get_crop_using_mask(m,np.array(image),w_slice = s_w,h_slice = s_h,p_h = p_h,p_w = p_w)\n                    np.save(os.path.join(p_im,str(im.id)),image)\n                    np.save(os.path.join(m_im,str(im.id)),mask)\n\n                self.labels.insert(len(self.labels.columns),'len',series)\n                self.labels.insert(len(self.labels.columns),'padding_w',padding_w)\n                self.labels.insert(len(self.labels.columns),'slice_w',slice_w)\n                self.labels.insert(len(self.labels.columns),'padding_h',padding_h)\n                self.labels.insert(len(self.labels.columns),'slice_h',slice_h)\n                self.labels.to_pickle(path = \"otsu\"+str(crop_h) + \"df.pkl\")\n                self.im_dir = p_im\n                self.mask_dir = m_im\n\n                #self.labels.insert(len(self.labels.columns),'crop_size',crop_size)\n            else:\n                #just read from the respective maps and the df\n                self.labels = pd.read_pickle(\"otsu\"+str(crop_h) + \"df.pkl\")\n                self.im_dir = p_im\n                self.mask_dir = m_im\n\n\n            \n        \n    def __len__(self):\n        #print(self.labels)\n        return self.labels.loc[:,'len'].sum()\n\n\n    #redoing getitem for the 3rd time because i don't know maths so we do only 0 or 90 rotation and 4 flips since flips are faster\n    def __getitem__(self,index):\n        l,index = self.convert_index(index)\n        image = np.load(os.path.join(self.im_dir, str(l.id) + \".npy\"))\n        mask = np.load(os.path.join(self.mask_dir, str(l.id) + \".npy\"))\n\n        if self.transform is not None:\n            rot,mirror,crop = self.convert_index_to_transform(index,l.len)\n            print(rot,mirror,crop)\n            #print(rot,mirror,crop)\n            if mirror == 1:\n                mask = mask[::-1,:]\n                image = image[::-1,:,:]\n            elif mirror == 2:\n                mask = mask[:,::-1]\n                image = image[:,::-1,:]\n            else:\n                mask = mask[::-1,::-1]\n                image = image[::-1,::-1,:]\n            #DONE replace this with own rotation if its faster\n            image,mask = self.get_crop_rotated(image,mask,crop,rot,self.crop_h)\n            #mask = np.rot90(mask,k=rot)\n            #image = np.rot90(image,k=rot)\n            #DONE this should be replaced to do the rotation and the crop extraciton inplace\n            #y,x = self.get_crop_coord(crop,image)\n            #im = np.copy(image[y:y+self.crop_h,x:x+self.crop_h])\n            #mask = np.copy(mask[y:y+self.crop_h,x:x+self.crop_h])\n            mask[mask == 255] = 1.0\n            image = self.transform(np.copy(image)/255.0)\n            mask = self.transform(np.copy(mask))\n\n\n        return image,mask#,l\n\n    #returns the actual crop of the mask and im\n    def get_crop_rotated(self,im,mask,crop_idx,rot,crp_h):\n        if rot == 0:\n            y = int((crop_idx //(im.shape[1]//crp_h))*crp_h)\n            x = int((crop_idx % (im.shape[1] // crp_h)) * crp_h)\n            #print(y,x,\"not flip\")\n            return im[y:y + crp_h, x:x + crp_h], mask[y:y + crp_h, x:x + crp_h]\n        else:\n            # rot1:(y,x) ->(im.shape[1]-x,y)\n            y = int((crop_idx % (im.shape[0] // crp_h)) * crp_h)\n            x = int((crop_idx // (im.shape[0] // crp_h)) * crp_h)\n            #print(y,x)\n            if im.shape[1]-1-x-crp_h<0:\n                return np.swapaxes(im[y:y + crp_h, im.shape[1] - 1 - x::-1], 0, 1), np.swapaxes(mask[y:y + crp_h, im.shape[1] - 1 - x::-1], 0, 1)\n            return np.swapaxes(im[y:y + crp_h, im.shape[1] - 1 - x:im.shape[1] - 1 - x - crp_h:-1], 0, 1), np.swapaxes(mask[y:y + crp_h, im.shape[1] - 1 - x:im.shape[1] - 1 - x - crp_h:-1], 0, 1)\n   \n    #image comes in already padded and cropped\n    def get_crop_coord(self,crop,im):\n        #first column index, then row index (columns are the inner loop and have higher freq)\n        y_l,x_l = im.shape[:2]\n        y = (crop // (x_l//self.crop_h))*self.crop_h\n        x = (crop % (x_l//self.crop_h))*self.crop_h\n\n        return int(y),int(x)\n        \n    \n    def convert_index(self,index):\n        suma = 0\n        for ix, l in self.labels.iterrows():\n            #print(l)\n            if index <suma+l.len:\n                return (l,index-suma)\n            else:\n                suma = suma + l.len\n\n    #changed so number of rots and mirrors is not hardcoded\n    def convert_index_to_transform(self,index,length):\n        crops = length/(self.no_rot*self.no_mirror)\n        rot = index //(self.no_rot*crops)\n        mirror = (index %(self.no_rot*crops))//crops\n        crop = (index %(self.no_rot*crops))%crops\n        \n        return rot,mirror,crop\n\n\n    #speeding up the crops without rotating and flipping the whole array\n    #flipping with indexing instead of np.flip is much faster around 4x faster\n    #need to test rotation + crop\n    #flip both axis and rotation is same as no flip and rotation and flip one axis is the same as flipping the other one -> only flip one axis and no flip\n","metadata":{"papermill":{"duration":0.118944,"end_time":"2022-08-13T13:44:09.961844","exception":false,"start_time":"2022-08-13T13:44:09.842900","status":"completed"},"tags":[],"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-08-23T09:23:55.669715Z","iopub.execute_input":"2022-08-23T09:23:55.670077Z","iopub.status.idle":"2022-08-23T09:23:55.712311Z","shell.execute_reply.started":"2022-08-23T09:23:55.670044Z","shell.execute_reply":"2022-08-23T09:23:55.711021Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"work_dir = \"./\"","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-08-23T09:23:55.713746Z","iopub.execute_input":"2022-08-23T09:23:55.714082Z","iopub.status.idle":"2022-08-23T09:23:55.730718Z","shell.execute_reply.started":"2022-08-23T09:23:55.714049Z","shell.execute_reply":"2022-08-23T09:23:55.729245Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass HistoDatasetTest(Dataset):\n    #crops will be rectangular \n    def __init__(self,im_dir,labels,transform=None,crop_h = -1,work_dir=\"./\"):\n        self.im_dir = im_dir\n        #self.mask_dir = mask_dir\n        self.transform = transform\n        self.labels = pd.read_csv(labels)\n        self.no_mirror = 1\n        self.no_rot = 1\n        self.crop_h = crop_h\n        self.crop_w = crop_h\n        self.rand = random.Random()\n        self.rand.seed(2022)\n        self.trans_path = work_dir\n        initialise = False\n        \n        \n        #need to add the pre transformed images this should be probs done befor training to cut down on costs\n        \n        if crop_h == -1:\n            #we are using whole images\n            series = [1]*len(self.labels)\n            self.labels.insert(len(self.labels.columns),'len',series)\n        else:\n            #\n            #example of image path: ./otsu250im and mask: ./otsu250mask and df is ./otsu250df.pickle\n            #\n            p_im = Path(self.trans_path+str(crop_h)+\"im\")\n            p_im.mkdir(parents=True, exist_ok=True)\n            #m_im = Path(self.path_otsu+str(crop_h)+\"mask\")\n            #m_im.mkdir(parents=True, exist_ok=True)\n            with os.scandir(p_im) as it:\n                initialise =  not any(it)\n            #we will be using crops\n            series = []\n            padding_h = []\n            padding_w = []\n            slice_w = []\n            slice_h = []\n            mask_included = []\n            crop_size = []\n            h_amount = []\n            w_amount = []\n            #indexes = []\n            #calc masks for tissue images and get the crop to calc amount of tiles(non overlaping with 0 padding)\n            #pr = False\n\n            for ix, im in self.labels.iterrows():\n                #################!!!!!!!!!!!!!!!!!!!!!!!!!###################\n                #       need to also shift the mask for padding amount      #\n                #    either shift the mask points or just save bin masks    #\n                #           mask will be saved as bin numpy array           #\n                #          we also save the dataframe of the thing          #\n                #############################################################\n\n                #loading mask and image\n                im_path = os.path.join(self.im_dir, str(im.id) + \".tiff\")\n                image = np.array(Image.open(im_path).convert(\"RGB\"))\n                x,y = image.shape[:2]\n\n\n                #\n\n                y_amount = image.shape[0] // self.crop_h #how many crops we can fit in without overlap, we want to get crops that include the tissue\n                #how much to pad height\n                p_h = self.crop_h - image.shape[0] % self.crop_h\n\n                x_amount = image.shape[1] // self.crop_w\n                #how much to pad width\n                p_w = self.crop_w - image.shape[1] % self.crop_w\n                im_len = self.no_mirror*self.no_rot*(y_amount + 1 if p_h>0 else 0 )*(x_amount + 1 if p_w>0 else 0 )\n\n                s_w = p_w//2#self.rand.randint(int(p_w//4),int(3*p_w//4))\n                s_h = p_h//2#self.rand.randint(int(p_h//4),int(3*p_h//4))\n                series.append(im_len)\n                padding_w.append(p_w)\n                slice_w.append(s_w)\n                padding_h.append(p_h)\n                slice_h.append(s_h)\n                h_amount.append(y_amount + 1 if p_h>0 else 0)\n                w_amount.append(x_amount + 1 if p_w>0 else 0 )\n\n                #creating the crops\n                mask = get_crop_using_mask(image,w_slice = s_w,h_slice = s_h,p_h = p_h,p_w = p_w)\n\n                #image = get_crop_using_mask(m,np.array(image),w_slice = s_w,h_slice = s_h,p_h = p_h,p_w = p_w)\n                np.save(os.path.join(p_im,str(im.id)),mask)\n                #np.save(os.path.join(m_im,str(im.id)),mask)\n\n            self.labels.insert(len(self.labels.columns),'len',series)\n            self.labels.insert(len(self.labels.columns),'padding_w',padding_w)\n            self.labels.insert(len(self.labels.columns),'slice_w',slice_w)\n            self.labels.insert(len(self.labels.columns),'padding_h',padding_h)\n            self.labels.insert(len(self.labels.columns),'slice_h',slice_h)\n            self.labels.insert(len(self.labels.columns),'h',h_amount)\n            self.labels.insert(len(self.labels.columns),'w',w_amount)\n            self.labels.to_pickle(path = work_dir+str(crop_h) + \"df.pkl\")\n            self.im_dir = p_im\n            #self.mask_dir = m_im\n\n            #self.labels.insert(len(self.labels.columns),'crop_size',crop_size)\n\n\n\n\n\n    def __len__(self):\n        #print(self.labels)\n        return self.labels.loc[:,'len'].sum()\n\n\n    #redoing getitem for the 3rd time because i don't know maths so we do only 0 or 90 rotation and 4 flips since flips are faster\n    def __getitem__(self,index):\n        l,index = self.convert_index(index)\n        image = np.load(os.path.join(self.im_dir, str(l.id) + \".npy\"))\n        h =  image.shape[0]\n        w =  image.shape[1]\n        #mask = np.load(os.path.join(self.mask_dir, str(l.id) + \".npy\"))\n        #print(\"image shape:\",image.shape)\n        #if self.transform is not None:\n        crop = self.convert_index_to_transform(index,l.len)\n        \n        #DONE replace this with own rotation if its faster\n        image = self.get_crop_rotated(image,crop,self.crop_h)\n        #print(\"Pre:\",image.shape)\n        image = np.copy(image)\n        #print(\"Mid:\",image.shape)\n        image = self.transform(image/255.0)\n        #print(\"Post:\",image.shape)\n        #mask = self.transform(np.copy(mask))\n\n        \n        return image,l.id,l.padding_w,l.slice_w,l.padding_h,l.slice_h,l.h,l.w#,image\n\n    #returns the actual crop of the mask and im\n    def get_crop_rotated(self,im,crop_idx,crp_h):\n        #print(\"Cropping:\",im.shape,\" \",crop_idx,' ',crp_h,'\\n')\n\n        y = int((crop_idx //(im.shape[1]//crp_h))*crp_h)\n        x = int((crop_idx % (im.shape[1] // crp_h)) * crp_h)\n        #print(crop_idx,y,x,\"not flip\")\n        return im[x:x + crp_h, y:y + crp_h,:]\n        \n\n    \n\n    #image comes in already padded and cropped\n    def get_crop_coord(self,crop,im):\n        #first column index, then row index (columns are the inner loop and have higher freq)\n        y_l,x_l = im.shape[:2]\n        y = (crop // (x_l//self.crop_h))*self.crop_h\n        x = (crop % (x_l//self.crop_h))*self.crop_h\n\n        return int(y),int(x)\n        \n    \n    def convert_index(self,index):\n        suma = 0\n        for ix, l in self.labels.iterrows():\n            #print(l)\n            if index <suma+l.len:\n                return (l,index-suma)\n            else:\n                suma = suma + l.len\n\n    #changed so number of rots and mirrors is not hardcoded\n    def convert_index_to_transform(self,index,length):\n        #crops = length/(self.no_rot*self.no_mirror)\n        \n        \n        return index#crops\n\n\n   ","metadata":{"papermill":{"duration":0.118944,"end_time":"2022-08-13T13:44:09.961844","exception":false,"start_time":"2022-08-13T13:44:09.842900","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-23T09:25:03.515614Z","iopub.execute_input":"2022-08-23T09:25:03.516075Z","iopub.status.idle":"2022-08-23T09:25:03.547360Z","shell.execute_reply.started":"2022-08-23T09:25:03.516037Z","shell.execute_reply":"2022-08-23T09:25:03.546456Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"testing reading the test images","metadata":{}},{"cell_type":"code","source":"labels = \"../input/hubmap-organ-segmentation/test.csv\"\nimages = \"../input/hubmap-organ-segmentation/test_images\"\nhist = HistoDatasetTest(images,labels,transform=None,crop_h = 250,work_dir=\"./\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-08-23T09:23:55.767530Z","iopub.execute_input":"2022-08-23T09:23:55.768606Z","iopub.status.idle":"2022-08-23T09:23:56.153577Z","shell.execute_reply.started":"2022-08-23T09:23:55.768548Z","shell.execute_reply":"2022-08-23T09:23:56.152487Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\"\"\"for i in range(len(hist)):\n    print(hist[i][1])\n    plt.imshow(hist[i][0])\n    plt.show()\"\"\"","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-08-23T09:23:56.154937Z","iopub.execute_input":"2022-08-23T09:23:56.155267Z","iopub.status.idle":"2022-08-23T09:23:56.163112Z","shell.execute_reply.started":"2022-08-23T09:23:56.155236Z","shell.execute_reply":"2022-08-23T09:23:56.161900Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"c = Image.open(mpath+f'/hubmap-organ-segmentation/train_images/10044.tiff')\nplt.figure(figsize=(20,20))\nplt.imshow(c)\nplt.show()","metadata":{"pycharm":{"name":"#%%\n"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2022-08-23T09:23:56.164987Z","iopub.execute_input":"2022-08-23T09:23:56.165717Z","iopub.status.idle":"2022-08-23T09:23:58.634887Z","shell.execute_reply.started":"2022-08-23T09:23:56.165669Z","shell.execute_reply":"2022-08-23T09:23:58.633628Z"},"collapsed":true,"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#print(len(hist))\n\"\"\"\npri = True\nn = 0\nfig ,axs = plt.subplots(1,10,figsize=(10,10))\nfor i in range(220,400):\n    #if pri:\n        #print(hist[i][2].id)\n    #print(hist[i][0].shape)\n    if(n == 10):\n        fig ,axs = plt.subplots(1,10,figsize=(10,10))\n        n = 0\n    axs[n].imshow(hist[i][0])\n    #plt.imshow(hist[i][0])\n    n = n+1\n    #input()\n\"\"\"","metadata":{"papermill":{"duration":0.088644,"end_time":"2022-08-13T13:46:57.881322","exception":false,"start_time":"2022-08-13T13:46:57.792678","status":"completed"},"tags":[],"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-08-23T09:23:58.636469Z","iopub.execute_input":"2022-08-23T09:23:58.636918Z","iopub.status.idle":"2022-08-23T09:23:58.643489Z","shell.execute_reply.started":"2022-08-23T09:23:58.636883Z","shell.execute_reply":"2022-08-23T09:23:58.642507Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def convert_index_to_transform(index,length):\n        crops = length/(4*4)\n        rot = index //(4*crops)\n        mirror = (index %(4*crops))//crops\n        crop = (index %(4*crops))%crops\n        \n        return rot,mirror,crop\n    \ndef convert_index(df,index):\n        suma = 0\n        for ix, l in df.iterrows():\n            #print(l)\n            if index <suma+l.len:\n                return (l.id,index-suma)\n            else:\n                suma = suma + l.len\n    \ndf = pd.DataFrame()\ndf.insert(0,'len',[15,12,33,14,3,4])\ndf.insert(0,'id',[0,1,2,3,4,5])\nfor i in range(df.loc[:,'len'].sum()):\n    print(convert_index(df,i))","metadata":{"papermill":{"duration":0.121855,"end_time":"2022-08-13T13:46:58.076928","exception":false,"start_time":"2022-08-13T13:46:57.955073","status":"completed"},"tags":[],"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2022-08-23T09:23:58.644965Z","iopub.execute_input":"2022-08-23T09:23:58.645694Z","iopub.status.idle":"2022-08-23T09:23:58.679358Z","shell.execute_reply.started":"2022-08-23T09:23:58.645647Z","shell.execute_reply":"2022-08-23T09:23:58.678417Z"},"collapsed":true,"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"utils","metadata":{"papermill":{"duration":0.076472,"end_time":"2022-08-13T13:46:58.228389","exception":false,"start_time":"2022-08-13T13:46:58.151917","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from torch.utils.data import DataLoader,Subset\nimport torchvision\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n\ndef load_checkpoint(filename):\n    print(\"LOADING MODEL\")\n    return torch.load(filename)\n\n\n\ndef get_loaders(\n    ds,\n    batch_size = 16,\n    shuffle_ds = False,\n    random_seed= 2022,\n    num_workers = 4,\n    pin_memory = True\n):\n    test_loader = DataLoader(\n        ds,\n        batch_size = batch_size,\n        num_workers = num_workers,\n        pin_memory = pin_memory,\n    ) \n    return (ds,test_loader)    \n\n","metadata":{"papermill":{"duration":0.097986,"end_time":"2022-08-13T13:46:58.401451","exception":false,"start_time":"2022-08-13T13:46:58.303465","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-23T09:25:07.083289Z","iopub.execute_input":"2022-08-23T09:25:07.083863Z","iopub.status.idle":"2022-08-23T09:25:07.091897Z","shell.execute_reply.started":"2022-08-23T09:25:07.083826Z","shell.execute_reply":"2022-08-23T09:25:07.090748Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"\nimport torch.optim as optim\nimport tqdm\n\n\nlr = 1e-4\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 1\nN_WORKERS = 2\nPIN_MEM = True\nLOAD_MODEL = True\nTEST_IM_DIR = \"../input/hubmap-organ-segmentation/test_images\"\nmodel_stats = torch.load('../input/base-model/UNET33_BCELogits_1(whole_data).pth.tar',map_location=torch.device(DEVICE))\n\n#def main(checkpoint = None):\ntrain_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n\n\nmodel = UNET(in_ch=3,out_ch=1).to(DEVICE)\nmodel = model.float()\nmodel.load_state_dict(model_stats['state_dict'])\n\nlabels = \"../input/hubmap-organ-segmentation/test.csv\"\nimages = \"../input/hubmap-organ-segmentation/test_images\"\nhist = HistoDatasetTest(images,labels,transform=train_transforms,crop_h = 250,work_dir=\"./\")\n\nds,test_loader = get_loaders(\n    hist,\n    #TRAIN_MASK_DIR,\n    #train_transforms,\n    #val_transforms,\n    batch_size=BATCH_SIZE,\n    num_workers = N_WORKERS,\n    pin_memory = PIN_MEM\n)\n#this will be updated every time we are done with an image\ncrops = []\nrle_encodings = []","metadata":{"papermill":{"duration":3.023515,"end_time":"2022-08-13T13:47:01.498553","exception":false,"start_time":"2022-08-13T13:46:58.475038","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-23T09:25:16.511699Z","iopub.execute_input":"2022-08-23T09:25:16.512895Z","iopub.status.idle":"2022-08-23T09:25:17.072884Z","shell.execute_reply.started":"2022-08-23T09:25:16.512826Z","shell.execute_reply":"2022-08-23T09:25:17.071874Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Server side testing\n#### Submission File\n\nIn order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n\nNote that, at the time of encoding, the mask should be binary, meaning the masks for all objects in an image are joined into a single large mask. A value of 0 should indicate pixels that are not masked, and a value of 1 will indicate pixels that are masked.\n\nThe competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\nThe file should contain a header and have the following format:","metadata":{}},{"cell_type":"code","source":"#create mask to rle conversion:\n#mask comes in cropped to the dimensions of the input\ndef bin_mask_to_rle(mask,id):\n    rle_encoded = f\"{id}, \"\n    #rows first from top to bottom\n    for j in mask.shape[1]:\n        starto = False\n        start_pix = 1\n        count = 0\n        for i in mask.shape[0]:\n            if mask[i,j] == 1:\n                count = count +1\n            else:\n                if count>0:\n                    rle_encoded.append(f\"{start_pix - count} {count} \")\n                    count = 0\n\n            start_pix = start_pix + 1\n    \n    return rle_encoded\n\n#stitiching the crops together\n#crops go from up to down and from left to right\n#padding is done only on the right and on the bottom(at max numbers on both axis)\n#concatenation is wrongly done\ndef stitch_crops(h,w,crops,padding_h_befor,padding_w_befor,padding_h_after,padding_w_after):\n    print(h,w)\n    l = 0\n    l_crops = []\n    for crop in crops:\n        if l == 0:\n            l_crops.append(crop)\n            #print(l_crops[0].shape)\n        else:\n            l_crops[-1] = np.concatenate((l_crops[-1],crop),axis=0)\n        l = l +1\n        #print(l)\n        if l == w:\n            #print(\"work?\")\n            l = 0\n    print(len(l_crops))\n    res = np.concatenate(l_crops,axis=1)\n    return res[padding_h_befor:res.shape[0]-padding_h_after,padding_w_befor:res.shape[1]-padding_w_after]\n    \n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-08-23T09:31:08.717190Z","iopub.execute_input":"2022-08-23T09:31:08.717683Z","iopub.status.idle":"2022-08-23T09:31:08.729712Z","shell.execute_reply.started":"2022-08-23T09:31:08.717637Z","shell.execute_reply":"2022-08-23T09:31:08.728445Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"#data = image,l.id,l.padding_w,l.slice_w,l.padding_h,l.slice_h,h,w\n\nmodel.eval()\nidenti = None\nprepad_w = 0\nprepad_h = 0\npostpad_w = 0\npostpad_h = 0\nh=0\nw= 0\nimages = []\ncrops = []\ncount = 0\nfor data in test_loader:\n    if identi is None or identi != data[1].item():\n        if identi != data[1].item() and identi is not None:\n            print(\"nope\")\n            images.append(stitch_crops(h,w,crops,prepad_h,prepad_w,postpad_h,postpad_w))\n        indenti = data[1].item()\n        prepad_w = data[3].item()\n        prepad_h = data[5].item() \n        postpad_w = data[2].item() - data[3].item() \n        postpad_h = data[4].item() - data[5].item() \n        h = data[6].item()\n        w = data[7].item()\n        \n    crops.append(data[0][0].detach().cpu().permute(1,2,0).numpy())\n    if len(crops) > 1 and crops[-1].shape != crops[-2].shape:\n        print(\"Prev shape:\",crops[-2].shape)\n        print(\"Cur shape:\",crops[-1].shape)\n        print(count)\n    count = count +1\n    \n        \n    \n    \nprint(len(crops)   )\nimages.append(stitch_crops(h,w,crops,prepad_h,prepad_w,postpad_h,postpad_w))\n#zadeve niso padane!!","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:31:12.276408Z","iopub.execute_input":"2022-08-23T09:31:12.276911Z","iopub.status.idle":"2022-08-23T09:31:13.623708Z","shell.execute_reply.started":"2022-08-23T09:31:12.276867Z","shell.execute_reply":"2022-08-23T09:31:13.621925Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"plt.imshow(images[0])","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:31:14.611239Z","iopub.execute_input":"2022-08-23T09:31:14.612222Z","iopub.status.idle":"2022-08-23T09:31:16.168958Z","shell.execute_reply.started":"2022-08-23T09:31:14.612164Z","shell.execute_reply":"2022-08-23T09:31:16.167444Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"a = np.array(Image.open('../input/hubmap-organ-segmentation/test_images/10078.tiff'))\nprint(a.shape)\n#c = torchvision.transforms.ToTensor()\n#im = get_crop_using_mask(a,w_slice=113,h_slice = 113,p_h = 227,p_w =227)\n#print(im.shape)\nplt.imshow(a)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:26:21.600820Z","iopub.execute_input":"2022-08-23T09:26:21.601376Z","iopub.status.idle":"2022-08-23T09:26:22.478098Z","shell.execute_reply.started":"2022-08-23T09:26:21.601317Z","shell.execute_reply":"2022-08-23T09:26:22.476893Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"TODO:\n-create the test for client side DONE\n-create the test for server side(conversion to poly and to rle)\n-make a more balnced dataset (50/50 mask without mask)\n-figure out if there is a better loss function than dice score\n-do non labeled segmentation and run the seg on that? sort of like pretraining\n\n\nWe will try 2 concepts:\n1. Predicting indiscriminant of organ with only one model\n2. Training a model for each organ seperatly","metadata":{}},{"cell_type":"markdown","source":"option:\njust rpedict the edge no need to predict the inside(use a thicker line for edge and just fill in the poly you get)","metadata":{"papermill":{"duration":0.076468,"end_time":"2022-08-13T13:48:34.046579","exception":false,"start_time":"2022-08-13T13:48:33.970111","status":"completed"},"tags":[]}},{"cell_type":"code","source":"torchvision.transforms.ToTensor(cxhxw)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:24:04.791343Z","iopub.status.idle":"2022-08-23T09:24:04.792278Z","shell.execute_reply.started":"2022-08-23T09:24:04.791976Z","shell.execute_reply":"2022-08-23T09:24:04.792006Z"},"trusted":true},"execution_count":null,"outputs":[]}]}